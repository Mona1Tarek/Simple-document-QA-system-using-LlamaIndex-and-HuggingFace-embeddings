# -*- coding: utf-8 -*-
"""LlamaIndex_p1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oP6yzs-pHdj40pXAew1PLuZpf3JSJtmu
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index

pip install llama-index

"""### lamaindex by default use the language model from openai"""

import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-embeddings-huggingface

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

doc = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(doc)
query_engine = index.as_query_engine()

response = query_engine.query("what is the Arab Unity?")
print(response)

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-llms-huggingface

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.huggingface import HuggingFaceLLM

from transformers import AutoTokenizer, AutoModelForCausalLM

# === Load the documents ===
documents = SimpleDirectoryReader("data").load_data()

# === Load embedding model ===
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5", device='cpu')

# === Fix GPT2 tokenizer pad token issue ===
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # âœ… Important fix to prevent IndexError

model = AutoModelForCausalLM.from_pretrained(model_name)

# === Set up Hugging Face LLM ===
llm = HuggingFaceLLM(
    model_name=model_name,
    tokenizer=tokenizer,
    model=model,
    context_window=1024,
    max_new_tokens=256,
    generate_kwargs={"top_k": 50},  # Removed top_p and temperature (not valid for GPT2)
    tokenizer_kwargs={"padding_side": "left"},
    messages_to_prompt=lambda messages: "\n".join([m.content for m in messages]),
    completion_to_prompt=lambda completion: completion
)

# === Build index ===
index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)

# === Create query engine ===
query_engine = index.as_query_engine(llm=llm)

# Query the index
response = query_engine.query("What is the official language of Egypt?")
print(response)

# Query the index
response = query_engine.query("What is the national economy in egypt based on")
print(response)